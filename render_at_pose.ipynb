{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5c365c8",
   "metadata": {},
   "source": [
    "# Render @ pose\n",
    "\n",
    "## Description\n",
    "\n",
    "This notebook is an example of loading network parameter weights from `.msgpack` file saved from a pre-trained Instant-NGP model and using it to bootstrap a model in python as a more convenient/lazy interface for downstream tasks. In this notebook an example of rendering at a specific pose is implemented.\n",
    "\n",
    "## Acknowledgement\n",
    "\n",
    "This notebook is inspired by [test.ipynb](https://github.com/kwea123/ngp_pl/blob/master/test.ipynb) and this [answer](https://github.com/NVlabs/instant-ngp/discussions/522#discussioncomment-3211571) by [@kwea123](https://github.com/kwea123)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec275a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure ipython to reload modules automatically when changed\n",
    "# Avoids having to restart the kernel every time a module is modified\n",
    "# https://stackoverflow.com/a/14390676\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c7b422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from models.networks import NGP\n",
    "from models.rendering import render\n",
    "from metrics import psnr\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import dataset_dict\n",
    "from datasets.ray_utils import get_ray_directions, get_rays\n",
    "from utils import load_ckpt\n",
    "from train import depth2img\n",
    "import imageio\n",
    "\n",
    "import msgpack\n",
    "from kornia.utils.grid import create_meshgrid3d\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [3, 3]\n",
    "plt.rcParams['figure.dpi'] = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b65edbc",
   "metadata": {},
   "source": [
    "# Load pre-trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186689fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_data = None\n",
    "\n",
    "# with open('/mnt/datasets/my_nerf_datasets/cuboids_and_joystick_360/transforms_base.msgpack', 'rb') as f:\n",
    "# with open('/mnt/datasets/xin_rosbag/transforms_base_aabb_4.msgpack', 'rb') as f:\n",
    "with open('/mnt/datasets/xin_rosbag/transforms_.msgpack', 'rb') as f:\n",
    "    pretrained_data = msgpack.loads(f.read())\n",
    "\n",
    "img_w, img_h = pretrained_data[\"snapshot\"][\"nerf\"][\"dataset\"][\"metadata\"][0][\"resolution\"]\n",
    "\n",
    "fx, fy = pretrained_data[\"snapshot\"][\"nerf\"][\"dataset\"][\"metadata\"][0][\"focal_length\"]\n",
    "cx, cy = pretrained_data[\"snapshot\"][\"nerf\"][\"dataset\"][\"metadata\"][0][\"principal_point\"]\n",
    "\n",
    "camera_intrinsics = torch.tensor([[fx,  0.0, cx * img_w],\n",
    "                                  [0.0, fy,  cy * img_h],\n",
    "                                  [0.0, 0.0, 1.0]])\n",
    "\n",
    "directions = get_ray_directions(img_h, img_w, torch.FloatTensor(camera_intrinsics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e3465f",
   "metadata": {},
   "source": [
    "# Scratchpad\n",
    "\n",
    "* `scale`=0.33\n",
    "* `aabb_scale`=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f18c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.density_grid.dtype)\n",
    "# print(128**3)\n",
    "# print(3*128**3//8)\n",
    "# print(\"snapshot_density_grid_size\", pretrained_data['snapshot'][\"density_grid_size\"])\n",
    "# print(\"3 * (128 ** 3) =\", 3 * (128 ** 3))\n",
    "# density_grid = np.frombuffer(pretrained_data['snapshot']['density_grid_binary'], dtype=np.float16).reshape((3, 128 ** 3))\n",
    "# print(\"read data size as float32: \", density_grid.shape)\n",
    "\n",
    "# print(pretrained_data.keys())\n",
    "\n",
    "# print(pretrained_data[\"rgb_network\"])\n",
    "# print(pretrained_data[\"snapshot\"].keys())\n",
    "# print(pretrained_data[\"snapshot\"][\"nerf\"].keys())\n",
    "# print(pretrained_data[\"snapshot\"][\"nerf\"][\"dataset\"][\"metadata\"][0][\"resolution\"])\n",
    "# print(torch.FloatTensor([pretrained_data[\"snapshot\"][\"nerf\"][\"dataset\"][\"xforms\"][0][\"start\"]]))\n",
    "\n",
    "# print(np.frombuffer(pretrained_data['snapshot']['params_binary'], dtype=np.uint8).shape[0]/13085152)\n",
    "# print(pretrained_data[\"encoding\"])\n",
    "# print(pretrained_data[\"snapshot\"][\"nerf\"][\"dataset\"].keys())\n",
    "# print(pretrained_data[\"snapshot\"][\"nerf\"][\"dataset\"][\"scale\"]) # 0.33\n",
    "# print(pretrained_data[\"snapshot\"][\"nerf\"][\"dataset\"][\"aabb_scale\"]) # 4\n",
    "# print(pretrained_data[\"snapshot\"][\"nerf\"][\"dataset\"][\"render_aabb\"])\n",
    "# print(pretrained_data[\"snapshot\"][\"nerf\"][\"dataset\"][\"render_aabb_to_local\"])\n",
    "# print(pretrained_data[\"snapshot\"][\"nerf\"][\"dataset\"][\"offset\"])\n",
    "\n",
    "print(pretrained_data.keys())\n",
    "print(pretrained_data[\"encoding\"])\n",
    "print(pretrained_data[\"network\"])\n",
    "print(pretrained_data[\"rgb_network\"][\"activation\"])\n",
    "print(pretrained_data[\"rgb_network\"][\"output_activation\"])\n",
    "\n",
    "print(pretrained_data[\"snapshot\"][\"aabb\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577a7a5e",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914db0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = float(pretrained_data['snapshot'][\"nerf\"][\"dataset\"][\"scale\"])\n",
    "aabb_scale = float(pretrained_data['snapshot'][\"nerf\"][\"dataset\"][\"aabb_scale\"])\n",
    "offset = torch.tensor([pretrained_data[\"snapshot\"][\"nerf\"][\"dataset\"][\"offset\"]])\n",
    "\n",
    "# print(offset)\n",
    "# print(scale)\n",
    "\n",
    "model = NGP(scale = aabb_scale, rgb_act='None', offset = offset).cuda()\n",
    "\n",
    "# Set network weights to the pre-trained weights from Instant-NGP\n",
    "xyz_encoder_size = model.state_dict()[\"xyz_encoder.params\"].shape[0]\n",
    "network_params = np.frombuffer(pretrained_data['snapshot']['params_binary'], dtype=np.float16)\n",
    "\n",
    "# print(\"n_params:\", pretrained_data['snapshot'][\"n_params\"])\n",
    "# print(\"network_params before:\", network_params.shape[0])\n",
    "\n",
    "# print(\"xyz_encoder.params.shape: \", model.xyz_encoder.params.shape)\n",
    "# print(\"rgb_net.params.shape: \", model.rgb_net.params.shape)\n",
    "# print(\"sum: \", model.xyz_encoder.params.shape[0] + model.rgb_net.params.shape[0])\n",
    "\n",
    "model.state_dict()[\"xyz_encoder.params\"][:] = torch.from_numpy(network_params[:xyz_encoder_size]).cuda()\n",
    "model.state_dict()[\"rgb_net.params\"][:] = torch.from_numpy(network_params[xyz_encoder_size:]).cuda()\n",
    "\n",
    "density_grid = np.frombuffer(pretrained_data['snapshot']['density_grid_binary'], dtype=np.float16).reshape((3, 128 ** 3))\n",
    "model.register_buffer('density_grid', torch.from_numpy(density_grid).cuda())\n",
    "\n",
    "model.register_buffer('grid_coords', create_meshgrid3d(128, 128, 128, False, dtype=torch.int32).reshape(-1, 3))\n",
    "\n",
    "model.update_density_grid(0.01*1024/3**0.5)\n",
    "model.update_density_grid(-0.01)\n",
    "#\n",
    "\n",
    "# model.center\n",
    "# print(model.xyz_min)\n",
    "# print(model.xyz_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bec021",
   "metadata": {},
   "source": [
    "# Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ee0a72",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pose = torch.FloatTensor([pretrained_data[\"snapshot\"][\"nerf\"][\"dataset\"][\"xforms\"][13][\"start\"]])\n",
    "\n",
    "pose = torch.FloatTensor([[1.0, 0.0, 0.0, 0.5],\n",
    "                         [0.0, -1.0, 0.0, 0.5],\n",
    "                         [0.0, 0.0, -1.0, 0.5]])\n",
    "\n",
    "rays_o, rays_d = get_rays(directions.cuda(), pose.cuda())\n",
    "\n",
    "results = render(model, rays_o, rays_d,\n",
    "                    **{'test_time': True,\n",
    "                        'T_threshold': 1e-2,\n",
    "                        'exp_step_factor': 1/256})\n",
    "\n",
    "plt.subplots(figsize=(15, 12))\n",
    "plt.tight_layout()\n",
    "plt.subplot(221)\n",
    "plt.title('pred')\n",
    "pred = results['rgb'].reshape(img_h, img_w, 3).cpu().numpy()\n",
    "plt.imshow(pred)\n",
    "plt.subplot(222)\n",
    "plt.title('depth')\n",
    "depth = results['depth'].reshape(img_h, img_w).cpu().numpy()\n",
    "depth_ = depth2img(depth)\n",
    "plt.imshow(depth_)\n",
    "plt.subplot(223)\n",
    "plt.title('opacity')\n",
    "plt.imshow(results['opacity'].reshape(img_h, img_w).cpu().numpy(), 'bone')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "72dacd8e6585549c1305a6e25348f86a81128ab60d964e2887b30fed7349d94f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
